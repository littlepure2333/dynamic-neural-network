<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://littlepure2333.github.io/dynamic-neural-network/feed.xml" rel="self" type="application/atom+xml" /><link href="https://littlepure2333.github.io/dynamic-neural-network/" rel="alternate" type="text/html" /><updated>2022-07-13T11:16:29-05:00</updated><id>https://littlepure2333.github.io/dynamic-neural-network/feed.xml</id><title type="html">动态神经网络研究组</title><subtitle>A blog of dynamic Neural Networks resarch group</subtitle><entry><title type="html">Active Pointly-Supervised Instance Segmentation</title><link href="https://littlepure2333.github.io/dynamic-neural-network/active%20learning/2022/07/06/active-point.html" rel="alternate" type="text/html" title="Active Pointly-Supervised Instance Segmentation" /><published>2022-07-06T00:00:00-05:00</published><updated>2022-07-06T00:00:00-05:00</updated><id>https://littlepure2333.github.io/dynamic-neural-network/active%20learning/2022/07/06/active-point</id><author><name></name></author><category term="Active Learning" /><summary type="html">视频 链接: https://pan.baidu.com/s/100LWOQv05phUPd3_K2NROg</summary></entry><entry><title type="html">Mixture of Experts (MoE): a brief introduction</title><link href="https://littlepure2333.github.io/dynamic-neural-network/moe/2022/06/22/MoE.html" rel="alternate" type="text/html" title="Mixture of Experts (MoE): a brief introduction" /><published>2022-06-22T00:00:00-05:00</published><updated>2022-06-22T00:00:00-05:00</updated><id>https://littlepure2333.github.io/dynamic-neural-network/moe/2022/06/22/MoE</id><author><name></name></author><category term="MoE" /><summary type="html">Mixture of Experts (MoE)</summary></entry><entry><title type="html">Masked Image Modeling for Self-supervised Learning</title><link href="https://littlepure2333.github.io/dynamic-neural-network/mim/2022/06/15/MIM.html" rel="alternate" type="text/html" title="Masked Image Modeling for Self-supervised Learning" /><published>2022-06-15T00:00:00-05:00</published><updated>2022-06-15T00:00:00-05:00</updated><id>https://littlepure2333.github.io/dynamic-neural-network/mim/2022/06/15/MIM</id><author><name></name></author><category term="MIM" /><summary type="html">Masked Image Modeling (MIM)</summary></entry><entry><title type="html">Differentiable Neural Architecture Search: Challenges and Solutions</title><link href="https://littlepure2333.github.io/dynamic-neural-network/nas/2022/04/20/Differentiable-NAS.html" rel="alternate" type="text/html" title="Differentiable Neural Architecture Search: Challenges and Solutions" /><published>2022-04-20T00:00:00-05:00</published><updated>2022-04-20T00:00:00-05:00</updated><id>https://littlepure2333.github.io/dynamic-neural-network/nas/2022/04/20/Differentiable-NAS</id><author><name></name></author><category term="NAS" /><summary type="html">大纲</summary></entry><entry><title type="html">dynamic token in ViT</title><link href="https://littlepure2333.github.io/dynamic-neural-network/dynamic%20transformer/2022/03/09/Dynamic-token-in-ViT.html" rel="alternate" type="text/html" title="dynamic token in ViT" /><published>2022-03-09T00:00:00-06:00</published><updated>2022-03-09T00:00:00-06:00</updated><id>https://littlepure2333.github.io/dynamic-neural-network/dynamic%20transformer/2022/03/09/Dynamic-token-in-ViT</id><author><name></name></author><category term="dynamic transformer" /><summary type="html">大纲 Pruning-based DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsificatio IA-RED$^2$ : Interpretability-Aware Redundancy Reduction for Vision Transformers EVIT: EXPEDITING VISION TRANSFORMERS VIA TOKEN REORGANIZATIONS ATS: Adaptive Token Sampling For Efficient Vision Transformers Different computational cost Dynamic Grained Encoder for Vision Transformers Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer Early exitting Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition Coarse-to-Fine Vision Transformer 视频链接 腾讯会议 PPT链接 百度网盘 提取码:dhbo</summary></entry></feed>